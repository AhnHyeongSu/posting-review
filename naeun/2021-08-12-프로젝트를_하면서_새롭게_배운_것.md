# 프로젝트를 하면서 새롭게 배운 내용들

# 1. 서브모듈

- 서브모듈은 Git 의 기능
- "Git 저장소 안에 다른 Git 저장소를 디렉토리로 분리해 넣는 것"

## 시작하기
```bash
  $ git submodule add 서브모듈경로
```
- 시작하면 `.gitsubmodule`이 생기며 설정정보가 들어간다.
```text
[submodule "test-submodule"]
	path = test-submodule
	url = https://github.com/knae11/test-submodule.git
```
- 서브모듈에 새로운 내용을 추가하고 싶다면 서브모듈에 파일을 추가하고, 해당 경로로 가서 commit, push 를 한다.
  - 서브모듈에서 git 관리는 따로 관리된다.
- 서브모듈에 새로운 내용이 반영되었다면, 본 프로젝트 커밋 목록에 서브모듈이 나타난다.
  ![서브모듈커밋](./images/2021-08-12-프로젝트/commit_submodule.png)

- 해당 파일을 commit, push 해야 원격 저장소에도 서브모듈이 적용된다.
- 서브모듈은 커밋의 해시값을 가리킨다.
  ![서브모듈remote](./images/2021-08-12-프로젝트/remote_submodule.png)

## 서브모듈 추가
- `git clone --recurse-submodules 서브모듈주소` 로 내부 값 전체를 클론해 추가할 수 있다.

## 서브모듈 내용 업데이트
- 원격 서브모듈에 새로운 내용을 반영했다고해서 본 프로젝트에 바로 반영되거나, 리모트 브랜치에 바로 반영되지 않는다.
  그 이유는 서브모듈은 브랜치를 가리키는 것이 아니라 커밋의 해시값을 가리키고 있기 때문이다.
- 방법1: **서브모듈 디렉토리로 가서** 원격 내용을 pull 하여 반영하면 된다. (원래 깃을 사용하듯이 반영하면 된다.)
- 방법2: **본 프로젝트에서** `git submodule update --remote 서브모듈이름` 로 서브모듈을 업데이트 시킨다.
  - 해당 명령어는 기본적으로 서브모듈의 `master` 브랜치에 checkout 후에 가져오는 방식이다.
  - update 할 때, 설정된 브랜치를 바꾸고 싶다면 `git config -f .gitmodules submodule.서브모듈이름.branch 서브모듈의브랜치` 명령어로
    바꿔줄 수 있다.
    - submodule 의 브랜치가 `main`으로 되어 있어서 초기에 `git submodule update` 가 되지 않았었다.
- update 후 본 프로젝트에도 commit, push 를 해야 본 프로젝트 원격레포에서도 업데이트한 서브모듈의 커밋 해시값을 가리키게 된다.

## 활용

- 프로젝트에서 private 레포짓토리를 서브모듈로 관리하여 설정정보나 비밀번호 등을 관리하고 있다.

## 참고자료

- [GIT 서브모듈 문서](https://git-scm.com/book/ko/v2/Git-%EB%8F%84%EA%B5%AC-%EC%84%9C%EB%B8%8C%EB%AA%A8%EB%93%88)

# 2. Elastic Stack

- ELK: Elasticsearch, Logstash, Kibana
  - Elasticsearch: 검색 및 분석엔진
  - Logstash: 서버 사이드 데이터 처리 파이프라인, 여러 소스의 데이터를 수집하여 변환한 뒤 elasticsearch 에 전송
  - Kibana: Elasticsearch 의 내용을 차트와 그래프를 이용해 데이터를 시각화
- Beats: 경량의 단일 목적 데이터 수집기 제품군
- Elastic Stack: EKL + Beat

## ELK 설치하기

- 프로젝트를 하면서는 별도의 EC2에 docker 에 설치하였다.
- [elk docker 설치](https://github.com/deviantony/docker-elk)
- elk 서버에 logstash, elasticsearch, kibana 를 설치
- logstash, kibana 는 elasticsearch 에 의존성을 가진다.

## Logstash

- filebeat 에서 로그를 받아 filter 로 분석하기

```yaml
input {
        beats {
                port => 포트번호
        }

        tcp {
                port => 포트번호
        }
}

filter {
        # backspace 제거: 정형화된 로그가 아니다 보니 \b 부분이 문제가 되었는데 없애줌
        mutate {
                gsub => ["log","[\b]", ""]
        }
        # grok 을 사용하여 원하는 방식대로 로그를 filter
        # grok 문법을 사용하며, log 필드에 들어온 값을 정규표현식과 비슷하게 파싱하여 json 형태로 보내준다.
        # %{해당패턴:필드로_묶을_이름:자료형} -> 필드로 묶을 이름을 지정하지 않으면 해당패턴 명으로 묶인다.
        grok {
                match => {"log" => ["Epoch %{NUMBER:currentEpoch:int}\/%{NUMBER:totalEpoch:int}
  %{GREEDYDATA}- loss: %{NUMBER:loss:float} - accuracy: %{NUMBER:accuracy:float}"] }
        }
        # jobId를 태그로 관리: [] 방식으로 json 경로의 key 접근가능
        mutate {
                rename => {"[attrs][tag]" => "jobId"}
        }
}

output {
        elasticsearch {
                hosts => 엘라스틱 포트
                # .. 각종 설정정보 넣어줌 ..
                # index 는 job 으로 클러스터링
                index => "job"
        }
}

```

## FileBeat

- 로그가 있는 서버에 설치
- Beats 의 제품군에는 여러가지가 존재하는데 그 중 로그파일 전용이 Filebeat

```yaml
filebeat.inputs:
- type: log
  # 도커 컨터이너로부터 쌓이는 로그의 경로 설정
  paths:
    - '/var/lib/docker/containers/*/*.log'
  # docker 의 로그는 json 형태로 쌓이는데 json.message_key 는 해당 key 에 해당하는 정보를 위로 올려준다.
  # 각 항목이 정확이 무얼 의미하는지 모르지만, docker 의 json 형태의 로그에서 생기는 문제를 풀어주는 역할을 했다.
  json.keys_under_root: true
  json.overwrite_keys: true
  json.add_error_key: true
  json.expand_keys: true
  json.message_key: log
  # 로그를 한줄한줄이 아닌 원하는 패턴의 단위로 묶어서 보내줄 수 있다.
  multiline.type: pattern
  multiline.pattern: '^Epoch'
  multiline.negate: true
  multiline.match: after
  # default 는 5초로 셋팅되어 있다. 묶어주는 로그 단위가 크면 중간에 의도한대로 나오지 않고 짤렸기 때문에 늘려주었다.
  multiline.timeout: 30
output.logstash:
  hosts: [로그스태시의 호스트 IP & PORT]
```

## Kibana

- 실제 kibana 를 사용하여 로그를 시각화 하지는 않았고, 프론트엔드 팀원들이 해주었다.
- kibana 는 elastic 에서 API 를 통해 봐야하는 로그정보들을 쉽게 시각화 해준다.

## ElasticSearch

- SpringBoot 를 사용한다면 의존성 추가로 쉽게 적용이 가능하다.
- gradle 설정에서 `implementation  'org.springframework.boot:spring-boot-starter-data-elasticsearch' `를
  추가
- Java 에서 Repository 처럼 쉽게 사용이 가능하다.
- Config 설정을 해준다.

```java
@Configuration
public class ElasticClientConfig extends AbstractElasticsearchConfiguration {
    @Override
    @Bean
    public RestHighLevelClient elasticsearchClient() {

        final ClientConfiguration clientConfiguration = ClientConfiguration.builder()
                .connectedTo(host)
                .withBasicAuth(userName, password)
                .build();

        return RestClients.create(clientConfiguration).rest();
    }
}

```

- ElasticsearchRepository 를 상속받아 Repository 처럼 사용가능하다.

```java
public interface LogRepository extends ElasticsearchRepository<Log, String> {
    List<Log> findByJobIdOrderByTime(Long jobId);
}
```

## 적용

- docker 에서 실행하며 쌓이는 로그를 파싱하여 읽어오기 위해 사용하였다.
- 모듈화된 로그가 아니라 원하는 형태로 로그를 보내고 파싱하는 부분에서 설정을 하며 많은 시도를 하였다.
- 기존 모듈화 되지 않은 로그를 분석하는데 사용하였지만 elasticsearch 는 검색엔진으로 사용될 때 큰 빛을 발하는 듯 하다.
- filebeat 에는 다양한 모듈이 존재하여(nginx, mysql 등) 쉽게 로그를 추출할 수 있는 듯 하다.

## 참고자료

- [what is elk](https://www.elastic.co/kr/what-is/elk-stack)
- [elk docker 설치](https://github.com/deviantony/docker-elk)
- [filebeat multiline](https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html#multiline)
- [spring-data docs elasticsearch](https://docs.spring.io/spring-data/elasticsearch/docs/current/reference/html/#reference)
